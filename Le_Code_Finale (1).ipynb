{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from sklearn import svm\n",
    "from itertools import groupby\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = []\n",
    "data_folder = Path('C:/Users/aagg9/datasets/datasets/train-articles')\n",
    "x = data_folder.glob(\"*.*\")\n",
    "files = [f for f in x]\n",
    "for file in files:\n",
    "    file_txt = open(file,encoding='utf-8')\n",
    "    line+=file_txt.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16965"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = []\n",
    "answers_folder = Path(\"C:/Users/aagg9/datasets/datasets/train-labels-SLC\")\n",
    "x = answers_folder.glob(\"*.*\")\n",
    "a = 0\n",
    "files = [f for f in x]\n",
    "#print (files)\n",
    "for file in files:\n",
    "    file_txt = open(file,encoding='utf-8')\n",
    "    lin=file_txt.readlines()\n",
    "    for i in lin:\n",
    "        if '-' in i:\n",
    "            binary.append(0)\n",
    "        else:\n",
    "            binary.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16965"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.DataFrame(line, columns=['sentence']).join(pd.DataFrame(binary, columns=['answer']))\n",
    "Data = Data.replace('',np.NaN)\n",
    "Data = Data.dropna()\n",
    "Data.index = range(len(Data))\n",
    "with open('Data.pkl', 'wb') as output_file: \n",
    "    pickle.dump(Data, output_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aas</th>\n",
       "      <th>ab</th>\n",
       "      <th>abadi</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>...</th>\n",
       "      <th>zubaydah</th>\n",
       "      <th>zurdos</th>\n",
       "      <th>zvloqcxl</th>\n",
       "      <th>zvolensy</th>\n",
       "      <th>zwerling</th>\n",
       "      <th>ça</th>\n",
       "      <th>être</th>\n",
       "      <th>óscar</th>\n",
       "      <th>über</th>\n",
       "      <th>ülés</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 17650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaa    aachen     aaron       aas        ab     abadi   abandon  \\\n",
       "0  0.001721  0.000484  0.000484  0.003387  0.000484  0.000000  0.002754   \n",
       "1  0.001265  0.000000  0.000000  0.000000  0.000000  0.000889  0.001898   \n",
       "\n",
       "   abandoned  abandoning  abandonment  ...  zubaydah    zurdos  zvloqcxl  \\\n",
       "0   0.002410    0.000689     0.000689  ...  0.000689  0.000000  0.000484   \n",
       "1   0.003795    0.000633     0.001265  ...  0.000633  0.000889  0.000000   \n",
       "\n",
       "   zvolensy  zwerling        ça      être     óscar      über      ülés  \n",
       "0  0.000484  0.000484  0.000484  0.000484  0.000484  0.000484  0.000968  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[2 rows x 17650 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Data.pkl', 'rb') as input_file:\n",
    "    Data = pickle.load(input_file)\n",
    "ans = [el for el,_ in groupby(sorted([str(tag) for tag in Data['answer']]))]\n",
    "Sentences_by_tag = {}\n",
    "i=0\n",
    "for Tag in ans:\n",
    "    Sentences_by_tag[Tag] = i\n",
    "    i=i+1\n",
    "Documents = []\n",
    "for Unique_tag in Sentences_by_tag.keys():\n",
    "    Sentences_by_tag[Unique_tag] = []\n",
    "    Sentences = ''\n",
    "    i = 0\n",
    "    for tag in Data['answer']:\n",
    "        if str(tag) == Unique_tag:\n",
    "            Sentences += ' '\n",
    "            Sentences += str(Data['sentence'][i])\n",
    "        i=i+1\n",
    "    Sentences_by_tag[Unique_tag] = Sentences\n",
    "    Documents.append(Sentences_by_tag[Unique_tag])\n",
    "\n",
    "for i in range(len(Documents)):\n",
    "    Documents[i] = re.sub(r\"[\\d+_]\", \"\", Documents[i], flags=re.UNICODE)\n",
    "vectorizer = TfidfVectorizer(lowercase = True, stop_words = stopwords.words('english')) \n",
    "matrix = vectorizer.fit_transform(Documents)\n",
    "\n",
    "\n",
    "Positions = {}\n",
    "j=0\n",
    "for word in vectorizer.get_feature_names():\n",
    "    Positions[word] = j\n",
    "    j=j+1\n",
    "amountWords = len(vectorizer.get_feature_names()) \n",
    "Tf_idf = pd.DataFrame(matrix.toarray(), columns = Positions.keys(), index = Sentences_by_tag.keys())\n",
    "Tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TF-IDF.pkl', 'wb') as output_file: \n",
    "    pickle.dump(Tf_idf, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('Positions.pkl', 'wb') as output_file: \n",
    "    pickle.dump(Positions, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TF-IDF.pkl', 'rb') as input_file:\n",
    "    Tf_idf = pickle.load(input_file)\n",
    "with open('Positions.pkl', 'rb') as input_file:\n",
    "    Positions = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция, превращающая предложение в список чисел в соответствии со схемой\n",
    "def To_vector (Sentence, Positions, Tf_Idf):\n",
    "    stolbec = np.zeros(len(Positions))\n",
    "    if isinstance(Sentence, str):\n",
    "        splittedSentence = re.split(r\"[,.!?:;\\\"\\'\\(\\)\\%\\=\\#\\&\\@\\$\\-\\*\\+\\>\\<\\/\\ \\s]\", Sentence)\n",
    "        splittedSentence = list(filter(None, splittedSentence)) \n",
    "        for word in splittedSentence:\n",
    "            try:\n",
    "                position = Positions[word.lower()]\n",
    "                stolbec[position] += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "#         if len(splittedSentence) > 1:\n",
    "#             for c in range (len(splittedSentence)-1):\n",
    "#                 pair = splittedSentence[c] + ' ' + splittedSentence[c+1]\n",
    "#                 try:\n",
    "#                     position = Positions[word.lower()]\n",
    "#                     stolbec[position] += 1\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "    return(normalize(Tf_Idf.dot(stolbec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Превращаем тестовую выборку в вектора\n",
    "def Create_vectors(Some_data, Positions, Tf_Idf):\n",
    "    Test = []\n",
    "    j=1\n",
    "    x = len(Some_data)\n",
    "    print(\"Vectorization in progress:\")\n",
    "    for i in Some_data:\n",
    "        Test.append(To_vector(i, Positions, Tf_Idf))\n",
    "        line = str(j) + '/' + str(x)\n",
    "        print(line, end=\"\\r\")\n",
    "        j += 1 \n",
    "    return(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Data['sentence'], Data['answer'], test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization in progress:\n",
      "13852/13852\r"
     ]
    }
   ],
   "source": [
    "vec_train_x=Create_vectors(X_train, Positions, Tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization in progress:\n",
      "2445/2445\r"
     ]
    }
   ],
   "source": [
    "vec_test_x=Create_vectors(X_test, Positions, Tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train.pkl', 'wb') as output_file: \n",
    "    pickle.dump(vec_train_x, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('X_test.pkl', 'wb') as output_file: \n",
    "    pickle.dump(vec_test_x, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('Y_train.pkl', 'wb') as output_file: \n",
    "    pickle.dump(Y_train, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('Y_test.pkl', 'wb') as output_file: \n",
    "    pickle.dump(Y_test, output_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train.pkl', 'rb') as input_file: \n",
    "    vec_train_x = pickle.load(input_file)\n",
    "with open('X_test.pkl', 'rb') as input_file: \n",
    "    vec_test_x = pickle.load(input_file)\n",
    "with open('Y_train.pkl', 'rb') as input_file: \n",
    "    Y_train = pickle.load(input_file)\n",
    "with open('Y_test.pkl', 'rb') as input_file: \n",
    "    Y_test = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import svm\n",
    "#model = svm.svc(parameters)\n",
    "#ОБУЧИТЬ МОДЕЛЬ (FIT)\n",
    "#ПРОВЕРИТЬ КАЧЕСТВО (score)\n",
    "#classification_report\n",
    "model = svm.SVC(kernel =  'linear')\n",
    "model.fit(vec_train_x, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7106555010106844"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(vec_train_x, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.71      0.83      2445\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2445\n",
      "   macro avg       0.50      0.35      0.41      2445\n",
      "weighted avg       1.00      0.71      0.83      2445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aagg9\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(model.predict(vec_test_x), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
